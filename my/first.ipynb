{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600439263922",
   "display_name": "Python 3.7.7 64-bit ('ml': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "导入 pytorch 库"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import torch"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "source": [
    "生成张量矩阵"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.]])\ntensor([[ 0.0547,  0.5870, -0.5169, -1.3087, -0.7692],\n        [ 0.4217,  0.1433, -1.5285, -0.1153, -0.0694],\n        [-1.9810, -1.5888, -0.7454, -0.6610, -1.0191],\n        [ 2.1525,  0.4409,  0.3594, -1.9272,  1.5251]])\ntorch.Size([4, 5]) torch.Size([4, 5])\n"
    }
   ],
   "source": [
    "height = 4\n",
    "width = 5\n",
    "type = torch.float\n",
    "x = torch.zeros(height, width, dtype = type)\n",
    "x = torch.rand(height, width)\n",
    "print(x.new_ones(height, width))\n",
    "print(torch.randn_like(x, dtype = torch.float))\n",
    "print(x.size(), x.shape)        # 返回 height, width"
   ]
  },
  {
   "source": [
    "利用 view 函数重塑张量"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([0.5915, 0.8367, 0.9751, 0.4737, 0.1089, 0.5773, 0.0857, 0.8377, 0.7627,\n        0.7098, 0.1919, 0.6099, 0.2173, 0.6880, 0.1656, 0.7736, 0.1788, 0.1442,\n        0.7041, 0.6251])\ntensor([[0.5915, 0.8367, 0.9751, 0.4737, 0.1089, 0.5773, 0.0857, 0.8377, 0.7627,\n         0.7098],\n        [0.1919, 0.6099, 0.2173, 0.6880, 0.1656, 0.7736, 0.1788, 0.1442, 0.7041,\n         0.6251]])\n"
    }
   ],
   "source": [
    "print(x.view(20))\n",
    "print(x.view(2, 10))"
   ]
  },
  {
   "source": [
    "ndarray 和 tensor 相互转化"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.from_numpy(tensor)    # tensor 转 ndarray"
   ]
  },
  {
   "source": [
    "使用 CUDA (CPU 和 GPU 操作)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[1.5470, 1.8688, 1.7033, 1.3549, 1.4193],\n        [1.7112, 1.2792, 1.8050, 1.4491, 1.0816],\n        [1.7271, 1.6421, 1.4443, 1.5189, 1.5889],\n        [1.5171, 1.7733, 1.2525, 1.9881, 1.1150]], device='cuda:0')\ntensor([[0.5470, 0.8688, 0.7033, 0.3549, 0.4193],\n        [0.7112, 0.2792, 0.8050, 0.4491, 0.0816],\n        [0.7271, 0.6421, 0.4443, 0.5189, 0.5889],\n        [0.5171, 0.7733, 0.2525, 0.9881, 0.1150]], dtype=torch.float64)\n"
    }
   ],
   "source": [
    "x = torch.rand(height, width)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    # 搬入 GPU 的两种方法\n",
    "    y = torch.ones_like(x, device = device)\n",
    "    x = x.to(device)\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(x.to(\"cpu\", torch.double))    # 搬回 CPU 以供处理 (在 CPU 才能处理)"
   ]
  },
  {
   "source": [
    "实现简单的一个 model \n",
    "- $ h = W_1X $\n",
    "- $ bet = max(0, h) $\n",
    "- $ y_{out} = W_2 bet $\n",
    "\n",
    "遵循 3 项:\n",
    "\n",
    "  1. **forward pass**\n",
    "  2. **loss**\n",
    "  3. **backward pass**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 5825537.79399452\n1 13025095.888369191\n2 26405999.43522036\n3 1455481.6157880486\n4 610692.0475780653\n5 364821.18168139213\n6 244382.677261854\n7 174770.89513128804\n8 130124.80237483399\n9 99765.83192699074\n10 78265.95633724422\n11 62547.98607320874\n12 50748.345429807945\n13 41729.73629864013\n14 34725.18272322697\n15 29202.493229743945\n16 24799.94836869903\n17 21253.9026531974\n18 18368.218801317886\n19 15998.17045942978\n20 14034.937311317832\n21 12395.62400570916\n22 11017.667822834504\n23 9849.18223258675\n24 8851.94895462026\n25 7993.592290459195\n26 7250.633480528266\n27 6603.269429166647\n28 6034.418399233932\n29 5533.6857348923495\n30 5092.548730993804\n31 4702.310094637323\n32 4353.712690533133\n33 4041.05693507614\n34 3759.707513177027\n35 3505.352324100504\n36 3274.8793425048743\n37 3065.510223138877\n38 2874.7271382204963\n39 2700.347798781125\n40 2540.6324022038652\n41 2394.128450718817\n42 2259.2552234851737\n43 2134.9367842027323\n44 2020.1713198889356\n45 1914.0120249637039\n46 1815.6434996649796\n47 1724.3865433673272\n48 1639.7027356503604\n49 1560.8483630391563\n50 1487.2601372766221\n51 1418.5703724405503\n52 1354.3648390296698\n53 1294.2763221030496\n54 1237.965995920349\n55 1185.1592049048104\n56 1135.5598173098974\n57 1088.971855113344\n58 1045.142734847559\n59 1003.8527533153596\n60 964.9341442174194\n61 928.1645652139081\n62 893.514644178553\n63 860.7877393227991\n64 829.7848055599762\n65 800.4234914690815\n66 772.5674615534981\n67 746.1265050102662\n68 720.9969051516159\n69 697.109606893891\n70 674.3813176111554\n71 652.723827599337\n72 632.0865122293607\n73 612.4474632485355\n74 593.9460149023514\n75 576.2572998951853\n76 559.3738381438068\n77 543.2240635582307\n78 527.7729105166649\n79 512.9701384770678\n80 498.80167142354\n81 485.24293715243004\n82 472.2249229999644\n83 459.7276984701024\n84 447.72781214944143\n85 436.19045244338724\n86 425.0985436927948\n87 414.4245454229556\n88 404.14335938563954\n89 394.24732426358844\n90 384.7082973812161\n91 375.5137318367596\n92 366.6436429179365\n93 358.0798622558226\n94 349.8123274909299\n95 341.8240369566867\n96 334.10825487958857\n97 326.64404199585965\n98 319.42403144765933\n99 312.43888632280715\n100 305.674689777557\n101 299.12631143641363\n102 292.7775114476872\n103 286.62808173828716\n104 280.6642728379022\n105 274.88103786363075\n106 269.2669071111485\n107 263.8200631823455\n108 258.5302954268285\n109 253.39307811766744\n110 248.3941832017452\n111 243.5371365504069\n112 238.8173271688296\n113 234.2277685143377\n114 229.76655083437123\n115 225.4259793433285\n116 221.20211101196634\n117 217.09132214783915\n118 213.08951102740633\n119 209.19272942842895\n120 205.39759189949922\n121 201.69925664939285\n122 198.09605716450733\n123 194.5826331623205\n124 191.15743635398053\n125 187.81766247567674\n126 184.56000801520258\n127 181.38193219625595\n128 178.34509471663424\n129 175.4644144392705\n130 172.65619611235286\n131 169.91760408742994\n132 167.24570679797006\n133 164.6381317803228\n134 162.09388529623658\n135 159.60944393598336\n136 157.1832474787446\n137 154.81331728254486\n138 152.4976113884136\n139 150.2345243814532\n140 148.02263334597708\n141 145.8602594662415\n142 143.74587909287686\n143 141.6784522806326\n144 139.65614721593448\n145 137.67752273783117\n146 135.74140609689925\n147 133.8470434482603\n148 131.99331925417192\n149 130.17766451160495\n150 128.40042301914278\n151 126.65984726659063\n152 124.95483408108754\n153 123.28478054728888\n154 121.64859130625544\n155 120.04541072502772\n156 118.47484820894871\n157 116.93605133726403\n158 115.42698862615786\n159 113.94748733470458\n160 112.49689700632413\n161 111.0744812422759\n162 109.67953285913767\n163 108.31411837635386\n164 106.97850639558702\n165 105.66867813898222\n166 104.3838934142601\n167 103.12348772485419\n168 101.88701422679534\n169 100.67370045202847\n170 99.48311940592328\n171 98.31415942044157\n172 97.16670082605935\n173 96.04005165165468\n174 94.93386593210934\n175 93.84747653687636\n176 92.7809032280243\n177 91.73284366220535\n178 90.70337826556971\n179 89.69181954149184\n180 88.69800328043681\n181 87.72135182077488\n182 86.76183927627909\n183 85.8184371172591\n184 84.89131252556314\n185 83.98025102456843\n186 83.08229998125503\n187 82.19955111206613\n188 81.33174373424805\n189 80.47848973753094\n190 79.63928951181244\n191 78.81374873379241\n192 78.00171293085599\n193 77.20286225821624\n194 76.41726548940105\n195 75.64397987866172\n196 74.88313391282136\n197 74.13437773761984\n198 73.39744463067014\n199 72.67219197175982\n200 71.95839890839095\n201 71.25560918287141\n202 70.56371890911045\n203 69.88243370441288\n204 69.21162196908293\n205 68.55103943424135\n206 67.9005859984023\n207 67.25988780034812\n208 66.628752942873\n209 66.00702078463397\n210 65.3945855863249\n211 64.7911922179816\n212 64.19675993346495\n213 63.61094776157562\n214 63.0336335018666\n215 62.4647074084642\n216 61.904002630041084\n217 61.35131724776211\n218 60.80659424432862\n219 60.26954842769831\n220 59.74006716999473\n221 59.21803133802749\n222 58.70331281655769\n223 58.195778707384676\n224 57.69531401009829\n225 57.20172198428985\n226 56.71484385651286\n227 56.23465040894256\n228 55.76119538112122\n229 55.293954154726705\n230 54.832994594713114\n231 54.37821717177545\n232 53.92955198955478\n233 53.48688306411241\n234 53.04994146998204\n235 52.618779652900116\n236 52.193213445511496\n237 51.773162646681016\n238 51.35859002042239\n239 50.949317503720636\n240 50.54530069436811\n241 50.14653794498251\n242 49.7527797923061\n243 49.36397558031827\n244 48.980083467191164\n245 48.60101610830394\n246 48.22664661368942\n247 47.856972422932444\n248 47.491889308552224\n249 47.13124104069773\n250 46.77500142556026\n251 46.42311162953561\n252 46.0755282023037\n253 45.73213139000518\n254 45.39286124243982\n255 45.057638213642804\n256 44.726423845515356\n257 44.39911178995884\n258 44.07566581005273\n259 43.75606705448274\n260 43.44020594806126\n261 43.12798245012573\n262 42.819373084516194\n263 42.51435552051394\n264 42.2128371842534\n265 41.91476179558937\n266 41.620095059962665\n267 41.328774538814116\n268 41.04072530995711\n269 40.755899186066614\n270 40.47426326772191\n271 40.19581140226505\n272 39.92054780211639\n273 39.64817591956704\n274 39.37879955336024\n275 39.112389211902936\n276 38.84888332126589\n277 38.58822015804523\n278 38.3305300354885\n279 38.075521380872246\n280 37.82320672661235\n281 37.573552798132525\n282 37.32654821804444\n283 37.08216052868897\n284 36.840382950996954\n285 36.60111971763104\n286 36.36433656560996\n287 36.13000944650527\n288 35.89812929528657\n289 35.66863557803614\n290 35.441478997511155\n291 35.21666989249928\n292 34.99412781277424\n293 34.77386749240672\n294 34.555801857224004\n295 34.33998340013564\n296 34.126301252294574\n297 33.914724687384556\n298 33.70526906621746\n299 33.49785211130363\n"
    }
   ],
   "source": [
    "# 使用最基础的来手动实现\n",
    "import numpy as np\n",
    "N, D_in, H, D_out = 32, 300, 100, 10\n",
    "times = 300\n",
    "\n",
    "X = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for i in range(times):\n",
    "    # 计算 forward pass\n",
    "    h = X.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # 计算 loss \n",
    "    loss = np.square(y - y_pred).sum()\n",
    "    print(i, loss)\n",
    "\n",
    "    # 计算 backward pass\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = X.T.dot(grad_h)\n",
    "\n",
    "    # 更新参数\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 3776156.5\n1 5528263.5\n2 14867354.0\n3 11923359.0\n4 168510.578125\n5 91333.40625\n6 57133.10546875\n7 39268.58203125\n8 28661.833984375\n9 21774.65234375\n10 17014.2265625\n11 13559.427734375\n12 10964.2236328125\n13 8971.068359375\n14 7412.0517578125\n15 6177.02783203125\n16 5184.91455078125\n17 4382.7607421875\n18 3727.31396484375\n19 3187.298095703125\n20 2739.06396484375\n21 2364.88037109375\n22 2050.697509765625\n23 1785.76611328125\n24 1561.045654296875\n25 1369.5546875\n26 1205.6883544921875\n27 1064.982666015625\n28 943.83935546875\n29 839.0029296875\n30 747.9459228515625\n31 668.59033203125\n32 599.287109375\n33 538.52978515625\n34 485.0826416015625\n35 437.9480895996094\n36 396.2491455078125\n37 359.2742614746094\n38 326.3955993652344\n39 297.0975341796875\n40 270.92645263671875\n41 247.4972381591797\n42 226.4735107421875\n43 207.57180786132812\n44 190.539306640625\n45 175.17869567871094\n46 161.28057861328125\n47 148.68836975097656\n48 137.25848388671875\n49 126.86995697021484\n50 117.41140747070312\n51 108.78533935546875\n52 100.9112548828125\n53 93.70448303222656\n54 87.10511016845703\n55 81.0510025024414\n56 75.49053192138672\n57 70.37824249267578\n58 65.669189453125\n59 61.32909393310547\n60 57.32275390625\n61 53.61979675292969\n62 50.196067810058594\n63 47.02411651611328\n64 44.08600616455078\n65 41.358428955078125\n66 38.822906494140625\n67 36.46586990356445\n68 34.27237319946289\n69 32.228694915771484\n70 30.32395362854004\n71 28.54694366455078\n72 26.886945724487305\n73 25.33637809753418\n74 23.886072158813477\n75 22.52859878540039\n76 21.257537841796875\n77 20.066402435302734\n78 18.949462890625\n79 17.90194320678711\n80 16.917949676513672\n81 15.993155479431152\n82 15.124154090881348\n83 14.307085037231445\n84 13.538021087646484\n85 12.81402587890625\n86 12.132381439208984\n87 11.489788055419922\n88 10.88393497467041\n89 10.312957763671875\n90 9.773893356323242\n91 9.265179634094238\n92 8.7849760055542\n93 8.331438064575195\n94 7.902769565582275\n95 7.497533798217773\n96 7.114475250244141\n97 6.752119064331055\n98 6.409268379211426\n99 6.084993362426758\n100 5.777878761291504\n101 5.486990451812744\n102 5.211730003356934\n103 4.950861930847168\n104 4.703665256500244\n105 4.469425201416016\n106 4.247399806976318\n107 4.036787033081055\n108 3.837150812149048\n109 3.647864818572998\n110 3.4681549072265625\n111 3.2976317405700684\n112 3.1358790397644043\n113 2.9823479652404785\n114 2.8365206718444824\n115 2.698129415512085\n116 2.5667037963867188\n117 2.441852331161499\n118 2.323261260986328\n119 2.2106664180755615\n120 2.103616237640381\n121 2.0019288063049316\n122 1.9052823781967163\n123 1.8134657144546509\n124 1.7261455059051514\n125 1.6431595087051392\n126 1.564283847808838\n127 1.4892430305480957\n128 1.417898178100586\n129 1.3500463962554932\n130 1.285555124282837\n131 1.2241547107696533\n132 1.1657805442810059\n133 1.1102385520935059\n134 1.0574049949645996\n135 1.0071048736572266\n136 0.9592616558074951\n137 0.9137699604034424\n138 0.8704167008399963\n139 0.8291799426078796\n140 0.7899476289749146\n141 0.7526006698608398\n142 0.7170426845550537\n143 0.6831772923469543\n144 0.650975227355957\n145 0.6202839016914368\n146 0.5910506844520569\n147 0.5632550120353699\n148 0.5367611646652222\n149 0.5115435123443604\n150 0.48753029108047485\n151 0.4646489918231964\n152 0.4428529143333435\n153 0.4220941960811615\n154 0.4023367166519165\n155 0.3835095465183258\n156 0.36556631326675415\n157 0.34848111867904663\n158 0.3322068452835083\n159 0.31670650839805603\n160 0.3019252121448517\n161 0.2878344655036926\n162 0.27443012595176697\n163 0.2616529166698456\n164 0.249466210603714\n165 0.23787003755569458\n166 0.2268083393573761\n167 0.21627455949783325\n168 0.20621980726718903\n169 0.1966610848903656\n170 0.1875266432762146\n171 0.1788388192653656\n172 0.17055338621139526\n173 0.16265220940113068\n174 0.15512481331825256\n175 0.14794136583805084\n176 0.14110210537910461\n177 0.13457556068897247\n178 0.12835632264614105\n179 0.12242887169122696\n180 0.11677859723567963\n181 0.11139415204524994\n182 0.10625714063644409\n183 0.10136035084724426\n184 0.09669146686792374\n185 0.09223484992980957\n186 0.08798978477716446\n187 0.08394213765859604\n188 0.08008158951997757\n189 0.07640153169631958\n190 0.0728885605931282\n191 0.06954295188188553\n192 0.06634935736656189\n193 0.06330263614654541\n194 0.06039761006832123\n195 0.0576288215816021\n196 0.05498773604631424\n197 0.05246679484844208\n198 0.050067491829395294\n199 0.047776490449905396\n200 0.04558637738227844\n201 0.04350090026855469\n202 0.041512567549943924\n203 0.03961890935897827\n204 0.03780737891793251\n205 0.03608022257685661\n206 0.034432247281074524\n207 0.03286059945821762\n208 0.031362250447273254\n209 0.029932992532849312\n210 0.028567401692271233\n211 0.0272662453353405\n212 0.026026736944913864\n213 0.024842362850904465\n214 0.02371082827448845\n215 0.022631192579865456\n216 0.021602008491754532\n217 0.02062036469578743\n218 0.01968218944966793\n219 0.018786799162626266\n220 0.017935654148459435\n221 0.017121851444244385\n222 0.016342714428901672\n223 0.015602510422468185\n224 0.014894921332597733\n225 0.01421915553510189\n226 0.013574128039181232\n227 0.012960154563188553\n228 0.012372025288641453\n229 0.011813720688223839\n230 0.011279424652457237\n231 0.010770105756819248\n232 0.010284751653671265\n233 0.00981883518397808\n234 0.009374557994306087\n235 0.008951768279075623\n236 0.008546384051442146\n237 0.008161408826708794\n238 0.007792380638420582\n239 0.007440936751663685\n240 0.007105994038283825\n241 0.006785653531551361\n242 0.0064796265214681625\n243 0.006189064122736454\n244 0.005910153035074472\n245 0.005645183380693197\n246 0.0053907884284853935\n247 0.005148506257683039\n248 0.004917966667562723\n249 0.004696149844676256\n250 0.004485748708248138\n251 0.004284239374101162\n252 0.004092071671038866\n253 0.003908929880708456\n254 0.003733609803020954\n255 0.00356619362719357\n256 0.0034058494493365288\n257 0.003253478556871414\n258 0.0031078485772013664\n259 0.002968899207189679\n260 0.002836664905771613\n261 0.002710096538066864\n262 0.0025890006218105555\n263 0.0024734893813729286\n264 0.0023632643278688192\n265 0.002257954329252243\n266 0.002157052280381322\n267 0.0020606357138603926\n268 0.0019689674954861403\n269 0.001881820964626968\n270 0.0017986383754760027\n271 0.001718820189125836\n272 0.0016416421858593822\n273 0.0015691817970946431\n274 0.0014995976816862822\n275 0.0014332743594422936\n276 0.001368877710774541\n277 0.0013086944818496704\n278 0.0012508649379014969\n279 0.0011956144589930773\n280 0.0011426532873883843\n281 0.0010917310137301683\n282 0.0010434029391035438\n283 0.0009973988635465503\n284 0.0009532978292554617\n285 0.0009113730629906058\n286 0.0008714053547009826\n287 0.0008330288110300899\n288 0.0007957935449667275\n289 0.000761179777327925\n290 0.0007277650875039399\n291 0.000696029863320291\n292 0.0006657807971350849\n293 0.0006366003071889281\n294 0.0006088242516852915\n295 0.0005819305661134422\n296 0.0005567154148593545\n297 0.0005325411912053823\n298 0.0005092528299428523\n299 0.0004869783006142825\n"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "N, D_in, H, D_out = 32, 300, 100, 10\n",
    "times = 300\n",
    "\n",
    "x = torch.randn(N, D_in, device = device, dtype = dtype)\n",
    "y = torch.randn(N, D_out, device = device, dtype = dtype)\n",
    "w1 = torch.randn(D_in, H, device = device, dtype = dtype, requires_grad = True)\n",
    "w2 = torch.randn(H, D_out, device = device, dtype = dtype, requires_grad = True)\n",
    "\n",
    "learning_rate = 1e-5\n",
    "for t in range(times):\n",
    "    # 计算 forward pass\n",
    "    y_pred = x.mm(w1).clamp(min = 0).mm(w2)\n",
    "\n",
    "    # 计算 loss \n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # 计算 backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # 更新参数\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0 286.2662048339844\n1 277.2135009765625\n2 268.703369140625\n3 260.7464294433594\n4 253.2162628173828\n5 246.04745483398438\n6 239.2170867919922\n7 232.6998291015625\n8 226.4536590576172\n9 220.44154357910156\n10 214.64041137695312\n11 209.0576171875\n12 203.70843505859375\n13 198.5474853515625\n14 193.55746459960938\n15 188.7219696044922\n16 184.05282592773438\n17 179.54685974121094\n18 175.1686248779297\n19 170.89974975585938\n20 166.7569580078125\n21 162.7478790283203\n22 158.85679626464844\n23 155.05438232421875\n24 151.32957458496094\n25 147.70286560058594\n26 144.1607208251953\n27 140.71640014648438\n28 137.3572235107422\n29 134.08804321289062\n30 130.8970947265625\n31 127.7657241821289\n32 124.70851135253906\n33 121.71721649169922\n34 118.7957534790039\n35 115.94145202636719\n36 113.14124298095703\n37 110.39445495605469\n38 107.7040023803711\n39 105.06673431396484\n40 102.48284912109375\n41 99.94824981689453\n42 97.45711517333984\n43 95.01776885986328\n44 92.6284408569336\n45 90.27799987792969\n46 87.97378540039062\n47 85.72235107421875\n48 83.52375030517578\n49 81.37161254882812\n50 79.26136779785156\n51 77.19657897949219\n52 75.18427276611328\n53 73.21099090576172\n54 71.2748794555664\n55 69.38111114501953\n56 67.52960968017578\n57 65.71949005126953\n58 63.94853591918945\n59 62.21571350097656\n60 60.521217346191406\n61 58.86861801147461\n62 57.254234313964844\n63 55.6800651550293\n64 54.14645004272461\n65 52.653141021728516\n66 51.194679260253906\n67 49.770938873291016\n68 48.382415771484375\n69 47.02954864501953\n70 45.708457946777344\n71 44.422428131103516\n72 43.16649627685547\n73 41.9451789855957\n74 40.75287628173828\n75 39.593711853027344\n76 38.46726989746094\n77 37.37212371826172\n78 36.3038215637207\n79 35.26627731323242\n80 34.25471878051758\n81 33.27207946777344\n82 32.31568908691406\n83 31.385740280151367\n84 30.48223304748535\n85 29.60384178161621\n86 28.751201629638672\n87 27.92112159729004\n88 27.114261627197266\n89 26.330257415771484\n90 25.56983757019043\n91 24.830814361572266\n92 24.112953186035156\n93 23.41568374633789\n94 22.73846435546875\n95 22.080862045288086\n96 21.44296646118164\n97 20.82433319091797\n98 20.223230361938477\n99 19.63956642150879\n100 19.072673797607422\n101 18.522212982177734\n102 17.988637924194336\n103 17.471355438232422\n104 16.96940803527832\n105 16.482574462890625\n106 16.010496139526367\n107 15.552878379821777\n108 15.1103515625\n109 14.681349754333496\n110 14.264698028564453\n111 13.860815048217773\n112 13.46981430053711\n113 13.089963912963867\n114 12.721353530883789\n115 12.364104270935059\n116 12.017733573913574\n117 11.682209014892578\n118 11.356879234313965\n119 11.041463851928711\n12010.735517501831055\n121 10.438547134399414\n122 10.15083122253418\n123 9.871654510498047\n124 9.600324630737305\n125 9.337517738342285\n126 9.082219123840332\n127 8.834709167480469\n128 8.594863891601562\n129 8.362244606018066\n130 8.136724472045898\n131 7.917121887207031\n132 7.704164981842041\n133 7.497776031494141\n134 7.297686576843262\n135 7.103743076324463\n136 6.915143966674805\n137 6.732471466064453\n138 6.555202960968018\n139 6.382988929748535\n140 6.215715408325195\n141 6.053351402282715\n142 5.895930290222168\n143 5.7426862716674805\n144 5.5943284034729\n145 5.450078964233398\n146 5.309876441955566\n147 5.173816204071045\n148 5.041722774505615\n149 4.913321018218994\n150 4.788564682006836\n151 4.667576789855957\n152 4.550114631652832\n153 4.435884475708008\n154 4.325142860412598\n155 4.217724800109863\n156 4.113448143005371\n157 4.011903762817383\n158 3.913264751434326\n159 3.8173587322235107\n160 3.7240567207336426\n161 3.6333682537078857\n162 3.545530319213867\n163 3.4600019454956055\n164 3.376913547515869\n165 3.295926332473755\n166 3.2172396183013916\n167 3.1405553817749023\n168 3.066040515899658\n169 2.9934825897216797\n170 2.922919511795044\n171 2.8543050289154053\n172 2.7876062393188477\n173 2.722496271133423\n174 2.659187078475952\n175 2.597419261932373\n176 2.5374410152435303\n177 2.4791176319122314\n178 2.4221982955932617\n179 2.366835117340088\n180 2.312917470932007\n181 2.2603793144226074\n182 2.2092487812042236\n183 2.1594035625457764\n184 2.1107983589172363\n185 2.063401699066162\n186 2.0172500610351562\n187 1.9722721576690674\n188 1.9284828901290894\n189 1.8857429027557373\n190 1.8440552949905396\n191 1.8034625053405762\n192 1.7638394832611084\n193 1.725170373916626\n194 1.6875332593917847\n195 1.6507893800735474\n196 1.6149940490722656\n197 1.5800857543945312\n198 1.545995831489563\n199 1.5127124786376953\n200 1.4803211688995361\n201 1.4487234354019165\n202 1.4179019927978516\n203 1.3878076076507568\n204 1.3584508895874023\n205 1.3297924995422363\n206 1.3018031120300293\n207 1.2745224237442017\n208 1.2479135990142822\n209 1.2219961881637573\n210 1.1966702938079834\n211 1.171992540359497\n212 1.1478620767593384\n213 1.1243155002593994\n214 1.101308822631836\n215 1.0788373947143555\n216 1.0569193363189697\n217 1.0355607271194458\n218 1.0147141218185425\n219 0.994307279586792\n220 0.9743557572364807\n221 0.9548081159591675\n222 0.9357627630233765\n223 0.9171208739280701\n224 0.8989258408546448\n225 0.8810935020446777\n226 0.8637188076972961\n227 0.8467475175857544\n228 0.830174446105957\n229 0.8139660358428955\n230 0.7981411814689636\n231 0.7826598286628723\n232 0.7675411105155945\n233 0.752739667892456\n234 0.7383458018302917\n235 0.7242497205734253\n236 0.710502028465271\n237 0.6970540285110474\n238 0.6839020252227783\n239 0.6710246205329895\n240 0.6584246754646301\n241 0.6461067795753479\n242 0.6340869069099426\n243 0.622303307056427\n244 0.6107369661331177\n245 0.5994248390197754\n246 0.5883563756942749\n247 0.577517032623291\n248 0.5668904781341553\n249 0.5565059185028076\n250 0.5463547110557556\n251 0.536404550075531\n252 0.5266508460044861\n253 0.5170950889587402\n254 0.5077264308929443\n255 0.49855268001556396\n256 0.48958319425582886\n257 0.48079267144203186\n258 0.47219178080558777\n259 0.4637577533721924\n260 0.4554915726184845\n261 0.4474036693572998\n262 0.43946290016174316\n263 0.4316778779029846\n264 0.4240483045578003\n265 0.4166058301925659\n266 0.40929633378982544\n267 0.4021278917789459\n268 0.3950853943824768\n269 0.388194739818573\n270 0.3814379870891571\n271 0.3748077154159546\n272 0.3683179020881653\n273 0.36196643114089966\n274 0.3557092547416687\n275 0.3495836853981018\n276 0.3435727059841156\n277 0.33767759799957275\n278 0.33190032839775085\n279 0.32623910903930664\n280 0.3206949830055237\n281 0.3152415156364441\n282 0.3098890483379364\n283 0.3046439290046692\n284 0.2994999885559082\n285 0.2944480776786804\n286 0.2894943952560425\n287 0.28464576601982117\n288 0.27988630533218384\n289 0.27520936727523804\n290 0.2706170976161957\n291 0.2661033868789673\n292 0.26165294647216797\n293 0.2572976052761078\n294 0.2530284821987152\n295 0.2488211989402771\n296 0.24469463527202606\n297 0.2406485676765442\n298 0.23666957020759583\n299 0.23276609182357788\n300 0.22893959283828735\n301 0.22518940269947052\n302 0.2214989960193634\n303 0.21787309646606445\n304 0.21431314945220947\n305 0.21082371473312378\n306 0.20739692449569702\n307 0.20403222739696503\n308 0.20073062181472778\n309 0.1974855214357376\n310 0.1942989081144333\n311 0.1911691278219223\n312 0.1880922168493271\n313 0.18507221341133118\n314 0.18211351335048676\n315 0.17919714748859406\n316 0.17633479833602905\n317 0.17352664470672607\n318 0.1707642674446106\n319 0.1680489480495453\n320 0.16538584232330322\n321 0.16277137398719788\n322 0.1601993590593338\n323 0.15767095983028412\n324 0.15518684685230255\n325 0.1527484506368637\n326 0.15035049617290497\n327 0.1479984074831009\n328 0.14568054676055908\n329 0.14340552687644958\n330 0.14116758108139038\n331 0.13896697759628296\n332 0.1368035078048706\n333 0.13468627631664276\n334 0.13259893655776978\n335 0.1305447816848755\n336 0.1285262405872345\n337 0.12654410302639008\n338 0.12459437549114227\n339 0.12268006056547165\n340 0.12079789489507675\n341 0.11894629150629044\n342 0.11712523549795151\n343 0.11533296853303909\n344 0.11357249319553375\n345 0.11184602230787277\n346 0.11014451086521149\n347 0.10846906155347824\n348 0.10682187974452972\n349 0.10520131886005402\n350 0.10360821336507797\n351 0.102045439183712\n352 0.1005062460899353\n353 0.09899147599935532\n354 0.09750029444694519\n355 0.09603367000818253\n356 0.09459004551172256\n357 0.09317365288734436\n358 0.09178031980991364\n359 0.09040926396846771\n360 0.08905963599681854\n361 0.08773037791252136\n362 0.08642397820949554\n363 0.0851399153470993\n364 0.0838753804564476\n365 0.08263178169727325\n366 0.08140802383422852\n367 0.0802026093006134\n368 0.07901588082313538\n369 0.07785262167453766\n370 0.07670550793409348\n371 0.07557648420333862\n372 0.07446537911891937\n373 0.07337112724781036\n374 0.07229561358690262\n375 0.07123924791812897\n376 0.07019665837287903\n377 0.06917070597410202\n378 0.06816200166940689\n379 0.06716932356357574\n380 0.06619314849376678\n381 0.06523162871599197\n382 0.06428453326225281\n383 0.06335214525461197\n384 0.062434565275907516\n385 0.061531178653240204\n386 0.06064387410879135\n387 0.059767886996269226\n388 0.05890548974275589\n389 0.058056965470314026\n390 0.05722137168049812\n391 0.05639876425266266\n392 0.05559112876653671\n393 0.05479350686073303\n394 0.05400789529085159\n395 0.05323461815714836\n396 0.05247383937239647\n397 0.05172639712691307\n398 0.05098894238471985\n399 0.050262536853551865\n400 0.04954693466424942\n401 0.048842646181583405\n402 0.04815016686916351\n403 0.0474676638841629\n404 0.046794235706329346\n405 0.04613201320171356\n406 0.045479197055101395\n407 0.044836122542619705\n408 0.04420449212193489\n409 0.043580882251262665\n410 0.04296697676181793\n411 0.04236216098070145\n412 0.04176592081785202\n413 0.041180096566677094\n414 0.040602535009384155\n415 0.0400334894657135\n416 0.039472587406635284\n417 0.03892044723033905\n418 0.03837691992521286\n419 0.037841420620679855\n420 0.037313513457775116\n421 0.036793045699596405\n422 0.03628017380833626\n423 0.03577619418501854\n424 0.0352795384824276\n425 0.03478945791721344\n426 0.03430689498782158\n427 0.0338309220969677\n428 0.03336234390735626\n429 0.032901741564273834\n430 0.032446760684251785\n431 0.03199867904186249\n432 0.03155680373311043\n433 0.031121456995606422\n434 0.030693894252181053\n435 0.030271537601947784\n436 0.029854925349354744\n437 0.029444601386785507\n438 0.02904023602604866\n439 0.028642307966947556\n440 0.028249552473425865\n441 0.027862634509801865\n442 0.027481509372591972\n443 0.027106281369924545\n444 0.02673671580851078\n445 0.02637135237455368\n446 0.026011548936367035\n447 0.025657134130597115\n448 0.02530756965279579\n449 0.02496408298611641\n450 0.02462439425289631\n451 0.024289820343255997\n452 0.023960402235388756\n453 0.023635677993297577\n454 0.02331610769033432\n455 0.023000400513410568\n456 0.02268913760781288\n457 0.022382579743862152\n458 0.022080300375819206\n459 0.021782614290714264\n460 0.02148871123790741\n461 0.021198900416493416\n462 0.02091367542743683\n463 0.02063269168138504\n464 0.02035566419363022\n465 0.02008223906159401\n466 0.019812416285276413\n467 0.01954667456448078\n468 0.01928507350385189\n469 0.01902679353952408\n470 0.018772035837173462\n471 0.01852085068821907\n472 0.018273210152983665\n473 0.018029779195785522\n474 0.017789114266633987\n475 0.01755177229642868\n476 0.017317837104201317\n477 0.017087427899241447\n478 0.016860397532582283\n479 0.01663602888584137\n480 0.016414739191532135\n481 0.01619688980281353\n482 0.015982190147042274\n483 0.015770375728607178\n484 0.015561366453766823\n485 0.015355112962424755\n486 0.015151984058320522\n487 0.014951975084841251\n488 0.014754286035895348\n489 0.014559321105480194\n490 0.014366937801241875\n491 0.014177643693983555\n492 0.013991175219416618\n493 0.01380695216357708\n494 0.013625120744109154\n495 0.013445716351270676\n496 0.01326925028115511\n497 0.013095051981508732\n498 0.012923110276460648\n499 0.012753470800817013\n"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 32, 300, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction = 'sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # 计算 forward pass\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 计算 loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.item())\n",
    "\n",
    "    # 计算 backward pass\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # 更新参数\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}